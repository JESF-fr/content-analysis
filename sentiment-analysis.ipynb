{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages à installer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch spacy transformers pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../scraping/scraping_gse.csv', sep='|') \n",
    "# text = str(df['article_content'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialisez le pipeline pour l'analyse de sentiment\n",
    "nlp = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment', device=\"cpu\")\n",
    "\n",
    "# Exemple de texte en français\n",
    "#text = \"J'adore ce produit ! C'est absolument incroyable. \" * 100  # Texte long pour l'exemple\n",
    "\n",
    "# Diviser le texte en segments de 512 tokens\n",
    "max_length = 512\n",
    "segments = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "# Analyse sentimentale pour chaque segment\n",
    "results = [nlp(segment) for segment in segments]\n",
    "\n",
    "# Afficher les résultats\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Segment {i + 1}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "fichiers = [x for x in listdir('../scraping') if x.endswith('.csv') and x.startswith('scraping_')]\n",
    "df_list = []\n",
    "for x in tqdm(fichiers):\n",
    "    file_path = f'../scraping/{x}'\n",
    "    df_csv = pd.read_csv(file_path, sep='|')\n",
    "    df_list.append(df_csv)\n",
    "# Concaténer les dataframes de la liste\n",
    "df_final = pd.concat(df_list)\n",
    "# Réinitialiser l'index du dataframe final\n",
    "df_final = df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.loc[df_final['website'] == 'https://www.droit-compta-gestion.fr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install jupyter ipywidgets nltk matplotlib seaborn plotly plotly-express wordcloud ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assurez-vous de télécharger les ressources nécessaires de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_final\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exemple de texte\n",
    "#text = my_df[\"article_content\"][0]\n",
    "def get_keywords(my_df):\n",
    "    keywords = []\n",
    "    keyword_rank = []\n",
    "    url_list = []\n",
    "    website_list = []\n",
    "\n",
    "    for i in tqdm(range(len(my_df))):\n",
    "        cleaned_text = clean_text(my_df['article_content'][i])\n",
    "\n",
    "        # Tokenisation et suppression des stop words\n",
    "        stop_words = set(stopwords.words('french'))\n",
    "        tokens = word_tokenize(cleaned_text)\n",
    "        filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Lemmatisation\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "        # Vérification des tokens lemmatisés\n",
    "        # print(lemmatized_tokens)\n",
    "\n",
    "        # Fréquence des mots\n",
    "        word_freq = Counter(lemmatized_tokens)\n",
    "        \n",
    "        for j in range(len(word_freq)):\n",
    "            keywords.append(word_freq.most_common()[j][0])\n",
    "            keyword_rank.append(word_freq.most_common()[j][1])\n",
    "            url_list.append(my_df['article_url'][i])\n",
    "            website_list.append(my_df['website'][i])\n",
    "        \n",
    "    return pd.DataFrame({\n",
    "        'article_url': url_list,\n",
    "        'keywords': keywords,\n",
    "        'keyword_rank': keyword_rank,\n",
    "        'website': website_list\n",
    "    })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy\n",
    "\n",
    "# Assurez-vous de télécharger le modèle français de spacy\n",
    "# python -m spacy download fr_core_news_sm\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "#Nettoyage du texte\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', str(text))  # Suppression des espaces multiples\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text))  # Suppression de la ponctuation\n",
    "    text = re.sub(r'\\d', '', text)  # Suppression des caractères numériques\n",
    "    text = text.lower()  # Conversion en minuscules\n",
    "    return text if len(text) >= 2 else ''\n",
    "\n",
    "def lemmatize_french(tokens):\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "def get_keywords(my_df):\n",
    "    keywords = []\n",
    "    keyword_rank = []\n",
    "    url_list = []\n",
    "    website_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "\n",
    "    for i in tqdm(range(len(my_df))):\n",
    "        cleaned_text = clean_text(my_df['article_content'][i])\n",
    "        tokens = word_tokenize(cleaned_text)\n",
    "        filtered_tokens = [word for word in tokens if word.lower() not in stop_words and len(word) > 2]\n",
    "        lemmatized_tokens = lemmatize_french(filtered_tokens)\n",
    "        word_freq = Counter(lemmatized_tokens)\n",
    "\n",
    "        for word, freq in word_freq.items():\n",
    "            keywords.append(word)\n",
    "            keyword_rank.append(freq)\n",
    "            url_list.append(my_df['article_url'][i])\n",
    "            website_list.append(my_df['website'][i])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'article_url': url_list,\n",
    "        'keywords': keywords,\n",
    "        'keyword_rank': keyword_rank,\n",
    "        'website': website_list\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('raw_scraping.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = get_keywords(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('keywords_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "ProfileReport(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['website'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Charger le modèle français\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # Traiter le texte avec spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extraire les mots-clés : noms, adjectifs, et entités nommées\n",
    "    keywords = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'ADJ'}:  # Noms, noms propres, adjectifs\n",
    "            keywords.add(token.lemma_)\n",
    "    \n",
    "    # Ajouter les entités nommées\n",
    "    for ent in doc.ents:\n",
    "        keywords.add(ent.text)\n",
    "    \n",
    "    return list(keywords)\n",
    "\n",
    "# Appliquer la fonction à chaque ligne du DataFrame\n",
    "tqdm.pandas(desc=\"Extraction des mots-clés\")\n",
    "df['keywords'] = df['article_content'].progress_apply(extract_keywords)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(df[['article_content', 'keywords']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy\n",
    "\n",
    "# Assurez-vous de télécharger les ressources nécessaires de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Charger le modèle français de spaCy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Nettoyage du texte\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', str(text))  # Suppression des espaces multiples\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text))  # Suppression de la ponctuation\n",
    "    text = re.sub(r'\\d', '', text)  # Suppression des caractères numériques\n",
    "    text = text.lower()  # Conversion en minuscules\n",
    "    return text if len(text) >= 2 else ''\n",
    "\n",
    "# Fonction pour extraire les mots-clés avec spaCy\n",
    "def extract_keywords(text):\n",
    "    # Traiter le texte avec spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extraire les mots-clés : noms, adjectifs, et entités nommées\n",
    "    keywords = set()\n",
    "    for token in doc:\n",
    "        if token.pos_ in {'NOUN', 'PROPN', 'ADJ'}:  # Noms, noms propres, adjectifs\n",
    "            keywords.add(token.lemma_)\n",
    "    \n",
    "    # Ajouter les entités nommées\n",
    "    for ent in doc.ents:\n",
    "        keywords.add(ent.text)\n",
    "    \n",
    "    return list(keywords)\n",
    "\n",
    "# Fonction pour obtenir les mots-clés de notre DataFrame\n",
    "def get_keywords(my_df):\n",
    "    keywords = []\n",
    "    keyword_rank = []\n",
    "    url_list = []\n",
    "    website_list = []\n",
    "\n",
    "    for i in tqdm(range(len(my_df))):\n",
    "        cleaned_text = clean_text(my_df['article_content'][i])\n",
    "\n",
    "        # Extraire les mots-clés avec spaCy\n",
    "        extracted_keywords = extract_keywords(cleaned_text)\n",
    "\n",
    "        # Fréquence des mots\n",
    "        word_freq = Counter(extracted_keywords)\n",
    "        \n",
    "        for word, freq in word_freq.items():\n",
    "            keywords.append(word)\n",
    "            keyword_rank.append(freq)\n",
    "            url_list.append(my_df['article_url'][i])\n",
    "            website_list.append(my_df['website'][i])\n",
    "        \n",
    "    return pd.DataFrame({\n",
    "        'article_url': url_list,\n",
    "        'keywords': keywords,\n",
    "        'keyword_rank': keyword_rank,\n",
    "        'website': website_list\n",
    "    })\n",
    "\n",
    "# Afficher les résultats\n",
    "keyword_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Assurez-vous de télécharger les ressources nécessaires de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Charger le modèle français de spaCy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "# Charger les stop words en français\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "# Nettoyage du texte\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', str(text))  # Suppression des espaces multiples\n",
    "    text = text.lower()  # Conversion en minuscules\n",
    "\n",
    "    # Séparer les mots et conserver la partie droite après l'apostrophe\n",
    "    words = text.split()\n",
    "    words = [word.split(\"’\")[-1] for word in words]  # Conserver la partie droite des mots avec apostrophes\n",
    "\n",
    "    # Rejoindre les mots nettoyés\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    text = re.sub(r\"[^\\w\\s]\", '', str(text))  # Suppression de la ponctuation sauf les apostrophes\n",
    "    text = re.sub(r'\\d', '', text)  # Suppression des caractères numériques\n",
    "    \n",
    "    \n",
    "    return text if len(text) >= 2 else ''\n",
    "\n",
    "# Fonction pour extraire les mots-clés avec spaCy\n",
    "def extract_keywords(text):\n",
    "    # Traiter le texte avec spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "# Extraire les mots-clés : noms, adjectifs, et entités nommées\n",
    "    keywords = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        # Filtrer les stop words des noun chunks\n",
    "        filtered_chunk = ' '.join([word for word in chunk.text.split() if word.lower() not in stop_words])\n",
    "        if filtered_chunk:\n",
    "            keywords.append(filtered_chunk)\n",
    "    \n",
    "    # Ajouter les entités nommées\n",
    "    for ent in doc.ents:\n",
    "        keywords.append(ent.text)\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "# Fonction pour obtenir les mots-clés d'un DataFrame\n",
    "def get_keywords(my_df):\n",
    "    keywords = []\n",
    "    keyword_rank = []\n",
    "    url_list = []\n",
    "    website_list = []\n",
    "\n",
    "    for i in tqdm(range(len(my_df))):\n",
    "        cleaned_text = clean_text(my_df['article_content'][i])\n",
    "\n",
    "        # Extraire les mots-clés avec spaCy\n",
    "        extracted_keywords = extract_keywords(cleaned_text)\n",
    "\n",
    "        # Fréquence des mots\n",
    "        word_freq = Counter(extracted_keywords)\n",
    "        \n",
    "        for word, freq in word_freq.items():\n",
    "            keywords.append(word)\n",
    "            keyword_rank.append(freq)\n",
    "            url_list.append(my_df['article_url'][i])\n",
    "            website_list.append(my_df['website'][i])\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'article_url': url_list,\n",
    "        'keywords': keywords,\n",
    "        'keyword_rank': keyword_rank,\n",
    "        'website': website_list\n",
    "    })\n",
    "\n",
    "# Exemple d'utilisation avec votre DataFrame\n",
    "keyword_df = get_keywords(df_final)\n",
    "\n",
    "# Afficher les résultats\n",
    "keyword_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_df.to_csv('keywords_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for index, row in df.iterrows():\n",
    "    texte = row['article_content']\n",
    "    doc = nlp(texte)\n",
    "    print(f\"Texte {index + 1}:\")\n",
    "    displacy.render(doc, style='ent', jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
