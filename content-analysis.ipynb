{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install stop-words faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -n .conda ipykernel --update-deps --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATABASES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "### SCRAPING\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "### VISUALISATION\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "### DIVERS\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "### DATACLEANING\n",
    "from collections import Counter\n",
    "\n",
    "### TEXT CLEANING\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "\n",
    "### NLP\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_websites = [\n",
    "    \"https://www.droit-compta-gestion.fr\",\n",
    "    \"https://www.entreprendre-maintenant.fr\"\n",
    "]\n",
    "output_dir = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Téléchargement du modèle pour Spacy (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection du modèle (modèle moyen de SpaCy) # ou md \n",
    "!spacy download fr_core_news_sm \n",
    "nlp = spacy.load(\"fr_core_news_sm\") #ou md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports des Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des données\n",
    "df = pd.read_csv('input/df_final.csv', sep='|')\n",
    "# Pour chaque site, on regroupe les contenus\n",
    "grouped = df[['website','article_canonical_url', 'article_content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des prépositions\n",
    "prepositions_fr = [\n",
    "    \"à\", \"après\", \"avant\", \"avec\", \"chez\", \"contre\", \"dans\", \"de\", \"depuis\",\n",
    "    \"derrière\", \"devant\", \"durant\", \"en\", \"entre\", \"envers\", \"excepté\", \"hors\",\n",
    "    \"jusque\", \"malgré\", \"par\", \"parmi\", \"pendant\", \"pour\", \"près\", \"sans\",\n",
    "    \"sauf\", \"selon\", \"sous\", \"suivant\", \"sur\", \"vers\", \"via\", \"concernant\",\n",
    "    \"outre\", \"quant à\", \"auprès de\", \"autour de\", \"à travers\", \"au-dessus de\",\n",
    "    \"au-dessous de\", \"au-delà de\", \"en dehors de\", \"en face de\", \"envers\",\n",
    "    \"grâce à\", \"loin de\", \"près de\", \"quant à\"\n",
    "]\n",
    "\n",
    "# Autres mots à éliminer\n",
    "stop_words_others = [\n",
    "    \"nan\", \"BIEN\", \"Cas\", \"faut\", \"Non\", \"Lire\", \"Plan\", \"ligne\", \"prendre\", \n",
    "    \"Choisir\", \"devez\", \"choix\", \"Place\", \"grâce\", 'url=\"http://www.comptazine.fr', \n",
    "    \"ça\", \"de france\", \"considérez\", \"post views\", \"ce\", \"contents1\", \n",
    "    \"quelles\", \"ii\", \"toogle\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\",\"g\", \"h\", \n",
    "    \"i\", \"k\", \"j\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \n",
    "    \"v\", \"w\", \"x\", \"y\", \"z\", \"plan de l'articleles\", \"choisissez\", \n",
    "    \"plan de l'articlequ'\", 'icon=\"icon', \"plan de l'\", \"navigation –\",\n",
    "    \"bts cg – cours\", \"fr\", \"px1\", \"processus 2\", \"processus-2-cours-bts\", \n",
    "    \"jusqu'\", \"n'hésitez\", \"utilisez\", \"pensez\", \"prenez\", \"puisqu'\"\n",
    "]\n",
    "\n",
    "# Liste des stopwords français\n",
    "stop_words_fr = set(stopwords.words('french')) | set(get_stop_words('french')) | set(prepositions_fr) | set(nlp.Defaults.stop_words) | set(stop_words_others)\n",
    "stop_words_fr = set(w.lower() for w in stop_words_fr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comptage des mots-clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comptage des mots-clés\n",
    "def compter_mots_cles(text):\n",
    "    words = text.lower().split()\n",
    "    words_filtrés = [word for word in words if word.isalpha() and word not in stop_words_fr]\n",
    "    compteur = Counter(words_filtrés)\n",
    "    return compteur\n",
    "df['keyword_count'] = df['article_content'].astype(str).apply(compter_mots_cles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame long\n",
    "rows = [\n",
    "    {'article_url': row.article_url, 'keyword': k, 'frequence': v}\n",
    "    for row in df.itertuples()\n",
    "    for k, v in row.keyword_count.items()\n",
    "]\n",
    "df_keywords = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['article_content'].astype(str).tolist()\n",
    "entities_list = []\n",
    "entities_freq_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Traitement en cours, la barre de progression va s'afficher après le premier batch…\")\n",
    "\n",
    "for doc in tqdm(nlp.pipe(texts, batch_size=125, n_process=18), total=len(texts), desc=\"Extraction des entités avec SpaCy\"):\n",
    "    ents = []\n",
    "    for ent in doc.ents:\n",
    "        text_lower = ent.text.lower()\n",
    "        if text_lower not in stop_words_fr:\n",
    "            ents.append(text_lower)\n",
    "    entities_list.append(ents)\n",
    "    entities_freq_list.append(dict(Counter(ents)))\n",
    "    \n",
    "df['entities'] = entities_list\n",
    "df['entities_freq'] = entities_freq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame des entités nommées\n",
    "df_entities = pd.DataFrame([\n",
    "    {'website': row.website, 'article_url': row.article_url, 'terme': k, 'frequence': v}\n",
    "    for row in df.itertuples()\n",
    "    for k, v in row.entities_freq.items()\n",
    "])\n",
    "df_entities['type'] = 'entité'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame des mots-clés\n",
    "df_keywords = pd.DataFrame([\n",
    "    {'website': row.website, 'article_url': row.article_url, 'terme': k, 'frequence': v}\n",
    "    for row in df.itertuples()\n",
    "    for k, v in row.keyword_count.items()\n",
    "])\n",
    "df_keywords['type'] = 'mot-clé'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_termes = pd.concat([df_entities, df_keywords], ignore_index=True)\n",
    "df_termes[\"terme\"] = df_termes[\"terme\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_termes.to_csv(f'{output_dir}/df_termes.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de préprocessing équivalente à celle utilisée par TfidfVectorizer\n",
    "def sklearn_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r\"\\b\\w\\w+\\b\", text)  # mots de 2+ lettres uniquement\n",
    "    return tokens\n",
    "\n",
    "# Nettoyage de tes stop_words selon le tokenizer de sklearn\n",
    "stop_words_clean = set()\n",
    "for word in stop_words_fr:\n",
    "    tokens = sklearn_tokenizer(str(word))\n",
    "    stop_words_clean.update(tokens)\n",
    "\n",
    "# Appliquer au vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words_clean, max_features=1000)\n",
    "\n",
    "# 1. TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words=list(stop_words_fr), max_features=1000)\n",
    "tfidf_sparse = vectorizer.fit_transform(df['article_content'].astype(str))\n",
    "\n",
    "# 2. Conversion en dense et float32\n",
    "tfidf_matrix = tfidf_sparse.toarray().astype(np.float32)\n",
    "\n",
    "# 3. Normalisation L2\n",
    "faiss.normalize_L2(tfidf_matrix)\n",
    "\n",
    "# 4. Index FAISS\n",
    "index = faiss.IndexFlatIP(tfidf_matrix.shape[1])\n",
    "index.add(tfidf_matrix)\n",
    "\n",
    "# 5. Recherche des voisins\n",
    "batch_size = 1000\n",
    "D_list, I_list = [], []\n",
    "for i in tqdm(range(0, tfidf_matrix.shape[0], batch_size), desc=\"Recherche FAISS\"):\n",
    "    D_batch, I_batch = index.search(tfidf_matrix[i:i+batch_size], k=6)\n",
    "    D_list.append(D_batch)\n",
    "    I_list.append(I_batch)\n",
    "D = np.vstack(D_list)\n",
    "I = np.vstack(I_list)\n",
    "\n",
    "# 6. Création des liens\n",
    "links = []\n",
    "for idx, (neighbors, sims) in tqdm(enumerate(zip(I, D)), total=len(df), desc=\"Détection des liens internes\"):\n",
    "    for neighbor_idx, sim in zip(neighbors[1:], sims[1:]):  # [1:] pour ignorer le self-match\n",
    "        links.append({\n",
    "            'article_website': df.iloc[idx]['website'],\n",
    "            'source_url': df.iloc[idx]['article_url'],\n",
    "            'target_url': df.iloc[neighbor_idx]['article_url'],\n",
    "            'similarity_score': sim * 100  # déjà normalisé\n",
    "        })\n",
    "\n",
    "df_links = pd.DataFrame(links)\n",
    "df_links = df_links[df_links['similarity_score'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liens_internes(html, site):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    liens = {}\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a.get('href', '')\n",
    "        if href.startswith(site):\n",
    "            liens[href.rstrip('/')] = a.get_text(strip=True)\n",
    "    return liens\n",
    "\n",
    "\n",
    "url_to_html = dict(zip(df['article_url'], df['article_raw_content']))\n",
    "url_to_site = dict(zip(df['article_url'], df['website']))\n",
    "\n",
    "\n",
    "liens_internes_dict = {\n",
    "    url: get_liens_internes(str(html), url_to_site.get(url, \"\"))\n",
    "    for url, html in tqdm(url_to_html.items(), desc=\"Extraction des liens internes\")\n",
    "}\n",
    "\n",
    "# 2. Vérification rapide pour chaque suggestion de lien\n",
    "df_links['lien_existant'] = [\n",
    "    row['target_url'] in liens_internes_dict.get(row['source_url'], set())\n",
    "    for _, row in tqdm(df_links.iterrows(), total=len(df_links), desc=\"Vérification des liens existants\")\n",
    "]\n",
    "\n",
    "# récupération des liens existants\n",
    "anchor_texts = []\n",
    "lien_existants = []\n",
    "\n",
    "for _, row in tqdm(df_links.iterrows(), total=len(df_links), desc=\"Mapping ancres / liens\"):\n",
    "    liens = liens_internes_dict.get(row['source_url'], {})\n",
    "    anchor = liens.get(row['target_url'].rstrip('/'))\n",
    "    anchor_texts.append(anchor)\n",
    "    lien_existants.append(anchor is not None)\n",
    "\n",
    "df_links['lien_existant'] = lien_existants\n",
    "df_links['anchor_text'] = anchor_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_internal_links = df_links[\n",
    "    df_links.apply(lambda row: str(row['target_url']).startswith(str(row['article_website'])), axis=1)\n",
    "]\n",
    "df_internal_links = df_internal_links[df_internal_links['target_url'] != df_internal_links['source_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_internal_links['lien_existant'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_internal_links.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_internal_links.to_csv(f'{output_dir}/df_internal_links.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df_internal_links[df_internal_links['lien_existant'] == True]\n",
    "\n",
    "df_articles_stats = pd.DataFrame({\n",
    "    'nb_outlinks_existants': df_valid['source_url'].value_counts(),\n",
    "    'nb_inlinks_existants': df_valid['target_url'].value_counts(),\n",
    "    'target_anchor_diversity': df_valid.groupby('target_url')['anchor_text'].nunique(),\n",
    "    'target_mean_similarity': df_valid.groupby('target_url')['similarity_score'].mean(),\n",
    "}).fillna(0).astype({'nb_outlinks_existants': int, 'nb_inlinks_existants': int, 'target_anchor_diversity': int})\n",
    "\n",
    "df_articles_stats['target_is_orphan'] = df_articles_stats['nb_inlinks_existants'] == 0\n",
    "df_articles_stats['target_depth'] = df_articles_stats.index.str.count('/') - 2  # ajuste selon ta structure d’URL\n",
    "\n",
    "df_articles_stats = df_articles_stats.reset_index().rename(columns={'index': 'article_url'})\n",
    "df_articles_stats['website'] = df_articles_stats['article_url'].str.split('https://').str[1].str.split('/').str[0]  # extraction du site à partir de l'URL\n",
    "df_articles_stats['website'] = df_articles_stats['website'].str.replace(r'www', r'https://www.', regex=False)  # assure que le site commence par https://www.\n",
    "df_articles_stats.to_csv(f'{output_dir}/df_articles_stats.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_stats['website'].isna().sum()  # Vérification des valeurs manquantes dans la colonne 'website'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['article_url', 'article_views']].to_csv(f'{output_dir}/df_views.csv', sep='|', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les liens les plus forts\n",
    "df_sub = df_internal_links[\n",
    "    (df_internal_links['similarity_score'] > 70) &\n",
    "    (df_internal_links['article_website'] == 'https://www.droit-compta-gestion.fr')\n",
    "]\n",
    "\n",
    "# Construire le graphe\n",
    "G = nx.from_pandas_edgelist(df_sub, 'source_url', 'target_url', ['similarity_score'])\n",
    "pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
    "\n",
    "# Préparer les arêtes avec épaisseur proportionnelle à la similarité\n",
    "edge_x, edge_y, edge_width = [], [], []\n",
    "for edge in G.edges(data=True):\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "    # Normaliser la similarité pour l'épaisseur\n",
    "    sim = edge[2]['similarity_score']\n",
    "    width = 1 + (sim - 70) / 10  # Ajuste ce facteur selon l'effet visuel souhaité\n",
    "    edge_width.append(width)\n",
    "\n",
    "# Pour Plotly, il faut une trace par épaisseur différente (astuce d’empilement)\n",
    "edge_traces = []\n",
    "for i, (x0, y0, x1, y1, width) in enumerate(zip(edge_x[::3], edge_y[::3], edge_x[1::3], edge_y[1::3], edge_width)):\n",
    "    edge_traces.append(go.Scatter(\n",
    "        x=[x0, x1], y=[y0, y1],\n",
    "        line=dict(width=width, color='#888'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    ))\n",
    "\n",
    "# Préparer les nœuds avec taille proportionnelle au degré\n",
    "node_x, node_y, node_text, node_size = [], [], [], []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_text.append(node)\n",
    "    degree = G.degree(node)\n",
    "    node_size.append(8 + 2 * degree)  # Ajuste le facteur selon l’effet visuel souhaité\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    text=node_text,\n",
    "    marker=dict(\n",
    "        size=node_size,\n",
    "        color='blue',\n",
    "        line_width=2\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=edge_traces + [node_trace])\n",
    "fig.update_layout(\n",
    "    title='Réseau de liens internes (épaisseur ∝ similarité, taille ∝ connexions)',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les liens les plus forts\n",
    "df_sub = df_internal_links[\n",
    "    (df_internal_links['similarity_score'] > 70) &\n",
    "    (df_internal_links['article_website'] == 'https://www.droit-compta-gestion.fr')\n",
    "]\n",
    "\n",
    "# Construire le graphe\n",
    "G = nx.from_pandas_edgelist(df_sub, 'source_url', 'target_url', ['similarity_score'])\n",
    "pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
    "\n",
    "# Préparer la normalisation pour la couleur des arêtes\n",
    "sim_scores = [G.edges[edge]['similarity_score'] for edge in G.edges()]\n",
    "norm = mcolors.Normalize(vmin=min(sim_scores), vmax=max(sim_scores))\n",
    "cmap = cm.get_cmap('plasma')  # Palette continue\n",
    "\n",
    "# Créer les traces d'arêtes avec couleur et épaisseur personnalisées\n",
    "edge_traces = []\n",
    "for edge in G.edges(data=True):\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    sim = edge[2]['similarity_score']\n",
    "    color = mcolors.to_hex(cmap(norm(sim)))\n",
    "    width = 1 + (sim - 70) / 10  # Ajuste ce facteur si besoin\n",
    "    edge_traces.append(go.Scatter(\n",
    "        x=[x0, x1], y=[y0, y1],\n",
    "        line=dict(width=width, color=color),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    ))\n",
    "\n",
    "# Calculer la taille des nœuds selon leur degré\n",
    "node_x, node_y, node_text, node_size = [], [], [], []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_text.append(node)\n",
    "    degree = G.degree(node)\n",
    "    node_size.append(8 + 2 * degree)  # Ajuste le facteur selon l'effet visuel souhaité\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    text=node_text,\n",
    "    marker=dict(\n",
    "        size=node_size,\n",
    "        color='blue',  # Tu peux aussi utiliser une couleur selon une autre métrique\n",
    "        line_width=2\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=edge_traces + [node_trace])\n",
    "fig.update_layout(\n",
    "    title='Réseau de liens internes (épaisseur & couleur ∝ similarité, taille ∝ connexions)',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"v\",    # vertical (par défaut)\n",
    "        yanchor=\"top\",      # ancrage vertical en haut\n",
    "        y=1,                # position verticale (1 = en haut)\n",
    "        xanchor=\"left\",     # ancrage horizontal à gauche de la légende\n",
    "        x=1.05              # position horizontale (>1 pour la placer à droite du graphe)\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Création du graphe depuis le dataframe\n",
    "G = nx.from_pandas_edgelist(df_sub, 'source_url', 'target_url', ['similarity_score'])\n",
    "\n",
    "# Position des nœuds\n",
    "pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
    "\n",
    "# Arêtes\n",
    "edge_x, edge_y = [], []\n",
    "for edge in G.edges():\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x, y=edge_y,\n",
    "    line=dict(width=0.5, color='#888'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines')\n",
    "\n",
    "# Nœuds\n",
    "node_x, node_y, node_text = [], [], []\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    node_text.append(node)  # ici tu peux personnaliser le texte affiché\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x, y=node_y,\n",
    "    mode='markers',\n",
    "    hoverinfo='text',\n",
    "    text=node_text,\n",
    "    marker=dict(\n",
    "        showscale=True,\n",
    "        colorscale='YlGnBu',\n",
    "        color=[],\n",
    "        size=10,\n",
    "        colorbar=dict(title='Nombre de connexions'),\n",
    "        line_width=2))\n",
    "\n",
    "fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                layout=go.Layout(\n",
    "                    title='Réseau de similarité des articles',\n",
    "                    showlegend=False,\n",
    "                    hovermode='closest',\n",
    "                    margin=dict(b=20,l=5,r=5,t=40),\n",
    "                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n",
    "               )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nouvelle fonction améliorée pour suggérer des ancres courtes\n",
    "def suggérer_ancres_courtes(source_text, target_termes, stop_words, n_max=5, longueur_max=4):\n",
    "    \"\"\"\n",
    "    Suggère des groupes nominaux courts issus du texte source en identifiant ceux qui\n",
    "    contiennent au moins un terme significatif du document cible.\n",
    "    \n",
    "    - n_max : nombre maximum d’ancres retournées\n",
    "    - longueur_max : nombre maximum de mots par ancre (groupe nominal)\n",
    "    \"\"\"\n",
    "    ancres_possibles = set()\n",
    "    doc = nlp(source_text)\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        tokens = [t.lemma_.lower() for t in chunk if not t.is_stop and t.is_alpha]\n",
    "        if not tokens or len(tokens) > longueur_max:\n",
    "            continue\n",
    "        if set(tokens) & target_termes:\n",
    "            ancre = \" \".join(tokens)\n",
    "            if all(t not in stop_words for t in tokens):\n",
    "                ancres_possibles.add(ancre)\n",
    "        if len(ancres_possibles) >= n_max:\n",
    "            break\n",
    "\n",
    "    return list(ancres_possibles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filtrage sur le site concerné\n",
    "mask = df_termes['website'].isin(selected_websites)\n",
    "df_termes_selected = df_termes[mask]\n",
    "\n",
    "df_links_selected = df_links[\n",
    "    (df_links['article_website'].isin(selected_websites)) &\n",
    "    (df_links['target_url'].apply(lambda url: any(url.startswith(site) for site in selected_websites)))\n",
    "]\n",
    "\n",
    "# 2. Dictionnaire {article_url: set(termes)}\n",
    "df_termes_grouped_dcg = df_termes_selected.groupby(\"article_url\")[\"terme\"].apply(set).to_dict()\n",
    "\n",
    "# 3. Génération des ancres suggérées\n",
    "anchor_suggestions = []\n",
    "\n",
    "for _, row in tqdm(df_links_selected.iterrows(), total=len(df_links_selected), desc=\"Génération des ancres courtes\"):\n",
    "    source_url = row['source_url']\n",
    "    target_url = row['target_url']\n",
    "\n",
    "    # Récupération du texte de l’article source\n",
    "    source_text_series = df[df['article_url'] == source_url]['article_content']\n",
    "    if source_text_series.empty:\n",
    "        anchor_suggestions.append([])\n",
    "        continue\n",
    "\n",
    "    source_text = str(source_text_series.values[0])\n",
    "    target_termes = df_termes_grouped_dcg.get(target_url, set())\n",
    "\n",
    "    ancres = suggérer_ancres_courtes(source_text, target_termes, stop_words_fr)\n",
    "    anchor_suggestions.append(ancres)\n",
    "\n",
    "df_links_selected['anchor_suggestions'] = anchor_suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des articles sources ayant eu des suggestions\n",
    "sources_avec_proposition = set(df_links_selected['source_url'].unique())\n",
    "\n",
    "# Liste de tous les articles du site\n",
    "tous_les_articles = set(df[df['website'] == 'https://www.droit-compta-gestion.fr']['article_url'].unique())\n",
    "\n",
    "# Quelles sources sont absentes ?\n",
    "sources_sans_proposition = tous_les_articles - sources_avec_proposition\n",
    "\n",
    "print(f\"Nombre d'articles sans suggestion : {len(sources_sans_proposition)}\")\n",
    "# Et pour voir lesquels :\n",
    "list(sources_sans_proposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links_selected[~df_links_selected['lien_existant']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded = df_links_selected.copy()\n",
    "df_exploded = df_exploded.explode('anchor_suggestions')\n",
    "df_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exploded.to_csv(f'{output_dir}/df_links_exploded.csv', sep='|', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
